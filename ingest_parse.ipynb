{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fe1c6a",
   "metadata": {},
   "source": [
    "# Setup\n",
    "For this to work the RPC URL needs to be from Helius as they are currently the only ones to support `getTransactionsForAddress`. It is possible to run this ingest & parse pipeline with free RPC (or RPC other than Helius), it would just need to use something like `getSignaturesForAddress` + `getTransaction` RPC calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "702cb4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "load_dotenv()\n",
    "RPC_URL = os.getenv(\"RPC_URL\")\n",
    "PROGRAM_ID = \"fUSioN9YKKSa3CUC2YUc4tPkHJ5Y6XW1yz8y6F7qWz9\"\n",
    "CALL_BATCH = 50\n",
    "\n",
    "data_dir = Path(\"data/raw\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475310a0",
   "metadata": {},
   "source": [
    "# Ingest\n",
    "Currently only ingesting Fusion transactions up to August 2nd 2025. To change modify the `blockTime` filter in `get_txs()` (UNIX timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f706cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txs(address, pagination = None, last_sig = None):\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"getTransactionsForAddress\",\n",
    "        \"params\": [\n",
    "            address,\n",
    "            {\n",
    "                'transactionDetails': 'full',\n",
    "                'encoding': 'jsonParsed',\n",
    "                'sortOrder': 'asc',\n",
    "                'limit': 100,\n",
    "                'filters': {\n",
    "                    'status': 'succeeded',\n",
    "                    'blockTime': {'lte': 1754107200} # Aug 2 2025\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if pagination:\n",
    "        payload['params'][1]['paginationToken'] = pagination\n",
    "    if last_sig:\n",
    "        payload['params'][1]['filters']['signature'] = {'gt': last_sig}\n",
    "\n",
    "    response = session.post(RPC_URL, json=payload)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c457167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrote 1 | Last block time: 2025-07-28, 02:27:42: 100%|██████████| 50/50 [00:22<00:00,  2.26it/s]\n",
      "Wrote 2 | Last block time: 2025-07-29, 16:45:28: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s]\n",
      "Wrote 3 | Last block time: 2025-07-30, 12:38:02: 100%|██████████| 50/50 [00:25<00:00,  1.95it/s]\n",
      "Wrote 4 | Last block time: 2025-07-31, 13:17:08: 100%|██████████| 50/50 [00:25<00:00,  1.98it/s]\n",
      "Wrote 5 | Last block time: 2025-08-01, 01:48:38: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s]\n",
      "Wrote 6 | Last block time: 2025-08-01, 23:58:35:  96%|█████████▌| 48/50 [00:22<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "txs = []\n",
    "pagination = None\n",
    "call_count = 0\n",
    "file_count = len(list((Path('data/raw').glob('*.json'))))\n",
    "last_sig = None\n",
    "if file_count > 0:\n",
    "    last_sig = json.loads(Path(f\"data/raw/{file_count}.json\").read_text())[-1]['transaction']['signatures'][0]\n",
    "\n",
    "pbar = tqdm(total=CALL_BATCH, mininterval=1)\n",
    "\n",
    "while True:\n",
    "    ts = get_txs(PROGRAM_ID, pagination, last_sig)\n",
    "    last_sig = None\n",
    "    call_count += 1\n",
    "    pbar.update(1)\n",
    "    if ts.get('result').get('data'):\n",
    "        txs.extend(ts['result']['data'])\n",
    "        pagination = ts['result']['paginationToken']\n",
    "        if not pagination: break\n",
    "    if not ts.get('result').get('data'):\n",
    "        break\n",
    "\n",
    "    if call_count % CALL_BATCH == 0:\n",
    "        file_count += 1\n",
    "        Path(f\"data/raw/{file_count}.json\").write_text(json.dumps(txs))\n",
    "        pbar.set_description(f\"Wrote {file_count} | Last block time: {datetime.fromtimestamp(txs[-1]['blockTime']).strftime('%Y-%m-%d, %H:%M:%S')}\")\n",
    "        txs = []\n",
    "        pbar.close()\n",
    "        pbar = tqdm(total=CALL_BATCH, mininterval=1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Dump any remaining txs\n",
    "if txs:\n",
    "    file_count += 1\n",
    "    Path(f\"data/raw/{file_count}.json\").write_text(json.dumps(txs))\n",
    "    pbar.set_description(f\"Wrote {file_count} | Last block time: {datetime.fromtimestamp(txs[-1]['blockTime']).strftime('%Y-%m-%d, %H:%M:%S')}\")\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb539046",
   "metadata": {},
   "source": [
    "# Parse Transactions\n",
    "Just extracting IX names and other meta here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e2af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating results...\n",
      "Writing 29,660 transactions metadata...\n",
      "Writing 33,543 fusion instructions...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from worker import process_file\n",
    "\n",
    "data_dir = Path('data/raw')\n",
    "out_dir = Path('data/parsed')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "paths = [str(p) for p in data_dir.glob('*.json')]\n",
    "\n",
    "all_metas = []\n",
    "all_fusion_ixs = []\n",
    "\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "with ProcessPoolExecutor() as ex:\n",
    "    results = list(tqdm(ex.map(process_file, paths), total=len(paths)))\n",
    "\n",
    "print(\"Aggregating results...\")\n",
    "\n",
    "for metas, ixs in results:\n",
    "    all_metas.extend(metas)\n",
    "    all_fusion_ixs.extend(ixs)\n",
    "\n",
    "\n",
    "print(f\"Writing {len(all_metas):,} transactions metadata...\")\n",
    "Path(out_dir / 'tx_meta.json').write_text(json.dumps(all_metas))\n",
    "\n",
    "\n",
    "print(f\"Writing {len(all_fusion_ixs):,} fusion instructions...\")\n",
    "Path(out_dir / 'ixs_fusion.json').write_text(json.dumps(all_fusion_ixs))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9677c71e",
   "metadata": {},
   "source": [
    "# Decode Instructions\n",
    "We are not parsing all possible instructions. You can expand the list of instructions to parse by adding entries to `ix_modules` (the key is instruction name as it appears in IDL and the value is the respective file from `fusionamm/instructions`)\n",
    "\n",
    "**NOTE**: openLimitOrder instructions need to be handeled for different versions. It is possible that more instructions need special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b5e11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33543/33543 [00:00<00:00, 52508.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tx: 5b6Y8XsBHeruy3N3oHmbzVz265EMXw1sKqifZ6Syg8BixMSr8X3T5CL5q3ddRtydsLRpo3QsKjUuextu6TvGvow2\n",
      "https://solscan.io/tx/5b6Y8XsBHeruy3N3oHmbzVz265EMXw1sKqifZ6Syg8BixMSr8X3T5CL5q3ddRtydsLRpo3QsKjUuextu6TvGvow2\n",
      "Instruction names: swap\n",
      "Instruction args: {'amount': 8000000, 'otherAmountThreshold': 0, 'sqrtPriceLimit': 79226673515401279992447579055, 'amountSpecifiedIsInput': True, 'aToB': False}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import base58\n",
    "from worker import INSTRUCTIONS\n",
    "ixs = json.loads(Path('data/parsed/ixs_fusion.json').read_text())\n",
    "\n",
    "ix_modules = {'swap': 'swap',\n",
    "         'increase_limit_order': 'increaseLimitOrder',\n",
    "         'open_limit_order': 'openLimitOrder',\n",
    "         'decrease_limit_order': 'decreaseLimitOrder',\n",
    "         'close_limit_order': 'closeLimitOrder',\n",
    "         'collect_fees': 'collectFees',\n",
    "         'increase_liquidity': 'increaseLiquidity',\n",
    "         'update_fees': 'updateFees',\n",
    "         'decrease_liquidity': 'decreaseLiquidity',\n",
    "         'open_position': 'openPosition',\n",
    "         'close_position': 'closePosition',\n",
    "         'two_hop_swap': 'twoHopSwap',\n",
    "         }\n",
    "\n",
    "for ix in tqdm(ixs):\n",
    "    ix_name = ix['ix_name']\n",
    "    if ix_name not in ix_modules:\n",
    "        continue\n",
    "\n",
    "    module = importlib.import_module(f'fusionamm.instructions.{ix_modules[ix_name]}')\n",
    "\n",
    "    if ix_name == 'open_limit_order':\n",
    "        if len(ix['ix_accounts']) == 9:\n",
    "            module = importlib.import_module(f'fusionamm.instructions.openLimitOrderOld')\n",
    "    \n",
    "    # Decode and parse args\n",
    "    if hasattr(module, 'layout'):\n",
    "        data_bytes = base58.b58decode(ix['ix_data'])\n",
    "        args_bytes = data_bytes[8:]  # Skip discriminator\n",
    "        parsed_args = dict(module.layout.parse(args_bytes))\n",
    "        parsed_args.pop('_io')\n",
    "        parsed_args.pop('remainingAccountsInfo') if 'remainingAccountsInfo' in parsed_args else None\n",
    "        ix['ix_args'] = parsed_args\n",
    "    else:\n",
    "        ix['ix_args'] = {}\n",
    "    \n",
    "    # Map accounts to names\n",
    "    account_names = INSTRUCTIONS[ix_name]['accounts']\n",
    "    ix['ix_accounts_named'] = dict(zip(account_names, ix['ix_accounts']))\n",
    "\n",
    "Path('data/parsed/ixs_fusion_parsed.json').write_text(json.dumps(ixs))\n",
    "\n",
    "\n",
    "print(f\"Sample tx: {ixs[0]['tx_signature']}\")\n",
    "print(f\"https://solscan.io/tx/{ixs[0]['tx_signature']}\")\n",
    "print(f\"Instruction names: {ixs[0]['ix_name']}\")\n",
    "print(f\"Instruction args: {ixs[0]['ix_args']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
